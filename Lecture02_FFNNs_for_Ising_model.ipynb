{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWxxmBLYKoln"
      },
      "source": [
        "# Lecture 2:\n",
        "## Machine Learning Ising Model\n",
        "\n",
        "E-Learning African International School on Quantum Science and Technology\n",
        "\n",
        "Sept 25, 2024\n",
        "\n",
        "Code by Lauren Hayward, Juan Carrasquilla, and Mohamed Hibat-Allah"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem set-up\n",
        "\n",
        "The objective of this problem is to train a feedforward neural network to classifying ferromagnetic (FM) and paramagnetic (PM) phases\n",
        "of the two-dimensional classical Ising model.\n",
        "You are encouraged to start by reading Reference https://arxiv.org/pdf/1605.01735.\n",
        "\n",
        "You have been given a dataset containing the following files:\n",
        "\n",
        "- `x_L30.txt`: $10\\,000$ samples of Ising spin configurations on a $30 \\times 30$ lattice, which were generated using Monte Carlo sampling.\n",
        "Each spin `up` state is stored as 1 and each spin `down` state is stored as 0. Note that this is different from what we have seen in the lectures. You can always relate to spins $\\pm 1$ by a linear transformation of the data, but this is not necessary.\n",
        "\n",
        "- `y_L30.txt`: The labels corresponding to each configuration.\n",
        "Each label represents whether a configuration was generated with $T<T_{\\text c}$ (label 0, corresponding to the FM phase in the thermodynamic limit)\n",
        "or $T>T_{\\text c}$ (label 1, corresponding to the PM phase in the thermodynamic limit).\n",
        "\n",
        "- `T_L30.txt`: The temperatures corresponding to each sample (measured in units of the coupling $J$).\n",
        "The temperature data file will not be used during the training, but will be used to study the accuracy of the network as a function of temperature in part 4).\n",
        "\n",
        "\n",
        "You can download the data on Google Colab using the following line:\n",
        "\n",
        "`!wget https://raw.githubusercontent.com/mhibatallah/ML-for-many-body-physics-course/main/FFNN_tutorial_data/filename.txt`\n",
        "\n",
        "where `filename.txt` = `x_L30.txt`, `y_L30.txt`, or `T_L30.txt` taken from this paper: https://www.nature.com/articles/nphys4035 (https://arxiv.org/abs/1605.01735). The goal of this problem is to train a feedforward neural network to learn the labels of this dataset.\n",
        "\n",
        "**Questions:**\n",
        "1. Run the code below and try to make sence of the logic of the code. You can use the tool `Gemini` if you have a question about a certain line of code.\n",
        "\n",
        "2. Plot accuracy and cost for training and validation data throughout the optimization using a FFNN with a preferred choice of hyperparameters. What is the accuracy on the testing data?\n",
        "\n",
        "3. Study the effect of different hyperparameters:\n",
        "  - Learning rate.\n",
        "  - Optimizer (gradient descent with and without momentum, Adam, ...).\n",
        "  - Activation functions.\n",
        "  - Number of neurons in the hidden layer.\n",
        "  - Number of hidden layers.\n",
        "\n",
        "4. Plot accuracy and output neurons values as a function of temperature. Provide an interpretation of the results. Compare your plots with Figure 1 of Reference (https://arxiv.org/abs/1605.01735). Discuss the behaviour that you observe as you approach the critical temperature $T_{\\text c}/J \\approx 2.269$.\n"
      ],
      "metadata": {
        "id": "W00z1I5e6Vhp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuUm4oZtGM77"
      },
      "source": [
        "## Create and plot the data set\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch"
      ],
      "metadata": {
        "id": "rrhlzxoO7GYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD4zSMIhdITs"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/mhibatallah/ML-for-many-body-physics-course/main/FFNN_tutorial_data/x_L30.txt\n",
        "!wget https://raw.githubusercontent.com/mhibatallah/ML-for-many-body-physics-course/main/FFNN_tutorial_data/y_L30.txt\n",
        "!wget https://raw.githubusercontent.com/mhibatallah/ML-for-many-body-physics-course/main/FFNN_tutorial_data/T_L30.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' #To use GPUs in PyTorch\n",
        "L=30\n",
        "n0 = L*L\n",
        "### Load data and shuffle it ###\n",
        "x = np.loadtxt( 'x_L%d.txt' %L, dtype=np.int32 )\n",
        "y = np.loadtxt( 'y_L%d.txt' %L, dtype=np.int32 )\n",
        "T = np.loadtxt( 'T_L%d.txt' %L, dtype=np.float64 )\n",
        "N_configs = x.shape[0]\n",
        "indices_shuffled = np.random.permutation(N_configs)\n",
        "x = x[indices_shuffled,:]\n",
        "y = y[indices_shuffled]\n",
        "T = T[indices_shuffled]\n",
        "\n",
        "### Divide into training, validation and testing datasets ###\n",
        "frac_train = 0.7 #Fraction of data used for training\n",
        "frac_validation = 0.2 #Fraction of data used for validation\n",
        "N_train = int(frac_train*N_configs)\n",
        "N_validation = int(frac_validation*N_configs)\n",
        "x_train = x[0:N_train,:]\n",
        "y_train = y[0:N_train]\n",
        "x_validation = x[N_train:(N_train+N_validation),:]\n",
        "y_validation = y[N_train:(N_train+N_validation)]\n",
        "x_test = x[(N_train+N_validation):,:]\n",
        "y_test = y[(N_train+N_validation):]\n",
        "T_test = T[(N_train+N_validation):]"
      ],
      "metadata": {
        "id": "O83u02EkJzEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnVlZ9Gf96KC"
      },
      "source": [
        "## Define the network architecture and training hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG5DlljlSEvB"
      },
      "source": [
        "%matplotlib inline\n",
        "from IPython import display\n",
        "\n",
        "import time\n",
        "\n",
        "class FeedforwardNN(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(FeedforwardNN, self).__init__()\n",
        "\n",
        "        #layer sizes:\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size  = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        #functions used within the Feedforward NN:\n",
        "        self.linear1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
        "        self.linear2 = torch.nn.Linear(self.hidden_size, self.output_size)\n",
        "        self.relu    = torch.nn.ReLU()\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "        self.softmax = torch.nn.Softmax()\n",
        "    def forward(self, x):\n",
        "        #Layer 1:\n",
        "        linear1_out = self.linear1(x)\n",
        "        #a1 = self.sigmoid(linear1_out)\n",
        "        a1 = self.relu(linear1_out)\n",
        "\n",
        "        #Layer 2:\n",
        "        linear2_out = self.linear2(a1)\n",
        "        #a2 = self.sigmoid(linear2_out)\n",
        "        a2 = self.softmax(linear2_out)\n",
        "\n",
        "        #Network output:\n",
        "        aL = a2\n",
        "\n",
        "        return aL\n",
        "\n",
        "input_size  = n0\n",
        "output_size = 2\n",
        "hidden_size = 4 #number of hidden units\n",
        "model = FeedforwardNN(input_size, hidden_size, output_size).to(device)\n",
        "### END OF MODIFICATIONS FOR PROBLEM 1c ###\n",
        "\n",
        "### Store the input data as a PyTorch tensor ###\n",
        "x_train = torch.tensor(x_train, dtype = torch.float).to(device) #add GPU support\n",
        "\n",
        "### One hot encoding ###\n",
        "y_onehot = np.zeros((y_train.size, output_size))\n",
        "y_onehot[np.arange(y_train.size),y_train] = 1\n",
        "y_onehot = torch.tensor(y_onehot, dtype = torch.float).to(device) #add GPU support\n",
        "\n",
        "### Use backpropagation to minimize the cost function using the gradient descent algorithm: ###\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "### Cost function: ###\n",
        "cost_func = torch.nn.MSELoss()\n",
        "# cost_func = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "N_epochs = 10000 # number of times to run gradient descent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOtexsTq-EfC"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ng0FlPl-YNf"
      },
      "source": [
        "epoch_list    = []\n",
        "cost_training = []\n",
        "acc_training  = []\n",
        "\n",
        "############ Function for plotting: ############\n",
        "def updatePlot():\n",
        "\n",
        "    ### Generate coordinates covering the whole plane: ###\n",
        "    padding = 0.1\n",
        "    spacing = 0.02\n",
        "    # x1_min, x1_max = x_train[:, 0].min() - padding, x_train[:, 0].max() + padding\n",
        "    # x2_min, x2_max = x_train[:, 1].min() - padding, x_train[:, 1].max() + padding\n",
        "    # x1_grid, x2_grid = np.meshgrid(np.arange(x1_min, x1_max, spacing),\n",
        "    #                      np.arange(x2_min, x2_max, spacing))\n",
        "\n",
        "    #torch_input = torch.tensor(np.c_[x1_grid.ravel(), x2_grid.ravel()], dtype = torch.float)\n",
        "    NN_output = model(x_train)\n",
        "    predicted_class = np.argmax(NN_output.cpu().detach().numpy(), axis=1)\n",
        "\n",
        "    ### Plot the classifier: ###\n",
        "    #plt.subplot(121)\n",
        "    #plt.contourf(x1_grid, x2_grid, predicted_class.reshape(x1_grid.shape), K, alpha=0.8)\n",
        "    #plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train, s=40)\n",
        "    #plt.xlim(x1_grid.min(), x1_grid.max())\n",
        "    #plt.ylim(x2_grid.min(), x2_grid.max())\n",
        "    #plt.xlabel(r'$x_1$')\n",
        "    #plt.ylabel(r'$x_2$')\n",
        "\n",
        "    ### Plot the cost function during training: ###\n",
        "    plt.subplot(222)\n",
        "    plt.plot(epoch_list,cost_training,'o-')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Training cost')\n",
        "\n",
        "    ### Plot the training accuracy: ###\n",
        "    plt.subplot(224)\n",
        "    plt.plot(epoch_list,acc_training,'o-')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Training accuracy')\n",
        "############ End of plotting function ############\n",
        "\n",
        "### Train for several epochs: ###\n",
        "for epoch in range(N_epochs):\n",
        "\n",
        "    optimizer.zero_grad() # sets the gradients to zero (necessary since PyTorch accumulates the gradients)\n",
        "    NN_output = model(x_train) # Neural network output\n",
        "    cost = cost_func(NN_output, y_onehot)\n",
        "    cost.backward() #computes the gradients\n",
        "    optimizer.step() #updating the parameters\n",
        "\n",
        "    ### Update the plot and print results every 500 epochs: ###\n",
        "    if epoch % 500 == 0:\n",
        "        predicted_class = np.argmax(NN_output.cpu().detach().numpy(), axis=1)\n",
        "        accuracy = np.mean(predicted_class == y_train)\n",
        "\n",
        "        epoch_list.append(epoch)\n",
        "        cost_training.append(cost.cpu().detach().numpy())\n",
        "        acc_training.append(accuracy)\n",
        "\n",
        "        ### Update the plot of the resulting classifier: ###\n",
        "        fig = plt.figure(2,figsize=(10,5))\n",
        "        fig.subplots_adjust(hspace=.3,wspace=.3)\n",
        "        plt.clf()\n",
        "        updatePlot()\n",
        "        display.display(plt.gcf())\n",
        "        print(\"Iteration %d:\\n  Training cost %f\\n  Training accuracy %f\\n\" % (epoch, cost, accuracy) )\n",
        "        display.clear_output(wait=True)\n",
        "        # time.sleep(0.1) #Uncomment this line if you want to slow down the rate of plot updates\n",
        "\n",
        "plt.savefig('results.pdf', bbox_inches=\"tight\")\n",
        "print(\"Final Training cost %f\\nFinal Training accuracy %f\\n\" % (cost, accuracy) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### TESTING DATA ###\n",
        "### Store the input data as a PyTorch tensor ###\n",
        "x_test = torch.tensor(x_test, dtype = torch.float).to(device) #add GPU support\n",
        "\n",
        "### One hot encoding ###\n",
        "y_test_onehot = np.zeros((y_test.size, output_size))\n",
        "y_test_onehot[np.arange(y_test.size),y_test] = 1\n",
        "y_test_onehot = torch.tensor(y_test_onehot, dtype = torch.float).to(device) #add GPU support\n",
        "\n",
        "NN_output_test = model(x_test)\n",
        "predicted_class_test = np.argmax(NN_output_test.cpu().detach().numpy(), axis=1)\n",
        "accuracy_test = np.mean(predicted_class_test == y_test)\n",
        "print(accuracy_test)"
      ],
      "metadata": {
        "id": "8QrYKVf-0vni"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}