{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWxxmBLYKoln"
      },
      "source": [
        "## Lecture 1:\n",
        "# Feedforward neural networks in PyTorch\n",
        "\n",
        "E-Learning African International School on Quantum Science and Technology\n",
        "\n",
        "Sept 18, 2024\n",
        "\n",
        "Code by Lauren Hayward, Juan Carrasquilla, and Mohamed Hibat Allah"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem set-up"
      ],
      "metadata": {
        "id": "kgOxSNm41TwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Today's objective is to become comfortable with using the Python package PyTorch to\n",
        "create and train a simple feedforward neural network for supervised learning.\n",
        "\n",
        "You will use and modify the code in this notebook.\n",
        "The first section of the code generates a random dataset of two-dimensional points with $\\texttt{K}$ branches.\n",
        "For example, when $\\texttt{K=3}$ this dataset might look as follows:"
      ],
      "metadata": {
        "id": "PDTF8RQxx3o6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!--- ![darts](https://drive.google.com/uc?id=1gQJiU6wZTlE8yE0ArjO6bFFMfgGCICN5) -->\n",
        "\n",
        "<h2 align=\"center\">\n",
        "<img src=\"https://drive.google.com/uc?id=1Pma18KS7rj9bGDjbmRSC0rGLZThcUyYX\"\n",
        "width=\"400\">\n",
        " </h2>\n"
      ],
      "metadata": {
        "id": "ay2Lxgmrzmqj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each datapoint $\\mathbf{x} = (x_1, x_2)$, the label is the branch index such that $y = 0, 1$ or 2 for the example above.\n",
        "Our goal is to implement a neural network capable of classifying the branches.\n",
        "\n",
        "This network will compare its output with labels in the so-called *one-hot encoding*.\n",
        "For a given label $\\texttt{y=k}$, the corresponding one-hot encoding is a $\\texttt{K}$-dimensional vector with all entries zero\n",
        "except for the $\\texttt{k}^\\text{th}$ entry (which has value 1).\n",
        "So when $\\texttt{K=3}$ the one-hot encodings for the labels are\n",
        "\\begin{equation*}\n",
        "0 \\rightarrow \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\qquad\n",
        "1 \\rightarrow \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\qquad\n",
        "2 \\rightarrow \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}.\n",
        "\\end{equation*}\n",
        "\n",
        "The code first defines the structure of the neural network\n",
        "and then uses the dataset to train this network.\n",
        "The code can generate two files: $\\texttt{spiral_data.pdf}$ (a plot of the dataset such as the one above)\n",
        "and $\\texttt{spiral_results.pdf}$ (which displays three plots illustrating the results of training)."
      ],
      "metadata": {
        "id": "bgRLdh6J0VmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises"
      ],
      "metadata": {
        "id": "hKE-m2a81XQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise \\#1:** Run the code in the cells below and look at how it attempts to classify the two-dimensional space.\n",
        "You should find that the resulting classifier separates the two-dimensional space using lines,\n",
        "and thus does a poor job of representing the data."
      ],
      "metadata": {
        "id": "oItw9B321ZHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise \\#2:** Look through the section of code labelled \"Define the network architecture and training hyperparameters\".\n",
        "Draw the neural network corresponding to the one in the code for the case of $\\texttt{K}$ branches.\n",
        "Pay particular attention to the number of neurons in each layer."
      ],
      "metadata": {
        "id": "npquiyB51xKr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise \\#3:** Add in a hidden layer with 4 neurons and study how this hidden layer changes the output.\n",
        "Draw the neural network in this case."
      ],
      "metadata": {
        "id": "txa2xYIs3QOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise \\#4:** Replace the sigmoid activation function on the first layer with a rectified linear unit (ReLU), and the one on the output layer with a softmax function.\n",
        "Study how the choices of activation functions change the output."
      ],
      "metadata": {
        "id": "O0hatK-r3WHU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise \\#5:** Study the effects of increasing and decreasing the $\\texttt{learning_rate}$ hyperparameter."
      ],
      "metadata": {
        "id": "z3aHmpnT3vuN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise \\#6:** Explain why the $\\texttt{K}$-dimensional one-hot encoding is useful. What do you think would happen if you used a one-dimensional label (such that $\\texttt{y}=0,1,\\ldots, \\texttt{K}-1$ or $\\texttt{K}$) instead?"
      ],
      "metadata": {
        "id": "QA1weDpDhVTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise \\#7:** What do you predict will happen to the training accuracy when you vary each of the following quantities? Repeat the training for different values of these quantities to see if your predictions are correct. Can you find any cases where the neural network appears to be overfitting?\n",
        "\n",
        "**a)** the number of neurons in the hidden layer,\n",
        "\n",
        "**b)** the magnitude of noise in the data, $\\texttt{mag_noise}$,\n",
        "\n",
        "**c)** the nubmer of different labels, $\\texttt{K}$."
      ],
      "metadata": {
        "id": "BeR8iWXairL1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuUm4oZtGM77"
      },
      "source": [
        "## Create and plot the data set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD4zSMIhdITs"
      },
      "source": [
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "N = 50 # number of points per branch\n",
        "K = 3  # number of branches\n",
        "\n",
        "N_train = N*K # total number of points in the training set\n",
        "x_train = np.zeros((N_train,2)) # matrix containing the 2-dimensional datapoints\n",
        "y_train = np.zeros(N_train, dtype='uint8') # labels (not in one-hot representation)\n",
        "\n",
        "mag_noise = 0.3  # controls how much noise gets added to the data\n",
        "dTheta    = 4    # difference in theta in each branch\n",
        "\n",
        "### Data generation: ###\n",
        "for j in range(K):\n",
        "  ix = range(N*j,N*(j+1))\n",
        "  r  = np.linspace(0.01,1,N) # radius\n",
        "  th = np.linspace(j*(2*np.pi)/K,j*(2*np.pi)/K + dTheta,N) + np.random.randn(N)*mag_noise # theta\n",
        "  x_train[ix] = np.c_[r*np.cos(th), r*np.sin(th)]\n",
        "  y_train[ix] = j\n",
        "\n",
        "### Plot the data set: ###\n",
        "fig = plt.figure(1, figsize=(5,5))\n",
        "plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train, s=40)#, cmap=plt.cm.Spectral)\n",
        "plt.xlim([-1,1])\n",
        "plt.ylim([-1,1])\n",
        "plt.xlabel(r'$x_1$')\n",
        "plt.ylabel(r'$x_2$')\n",
        "plt.savefig('spiral_data.pdf', bbox_inches=\"tight\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmyO880M8mRO"
      },
      "source": [
        "#Run this cell if you want to save a pdf plot of the dataset:\n",
        "#files.download('spiral_data.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnVlZ9Gf96KC"
      },
      "source": [
        "## Define the network architecture and training hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG5DlljlSEvB"
      },
      "source": [
        "%matplotlib inline\n",
        "from IPython import display\n",
        "\n",
        "import time\n",
        "import torch\n",
        "\n",
        "class FeedforwardNN(torch.nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(FeedforwardNN, self).__init__()\n",
        "\n",
        "        #layer sizes:\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        #functions used within the Feedforward NN:\n",
        "        self.linear1 = torch.nn.Linear(self.input_size, self.output_size)\n",
        "        self.relu    = torch.nn.ReLU()\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "        self.softmax = torch.nn.Softmax()\n",
        "    def forward(self, x):\n",
        "        #Layer 1:\n",
        "        linear1_out = self.linear1(x)\n",
        "        a1 = self.sigmoid(linear1_out)\n",
        "\n",
        "        #Network output:\n",
        "        aL = a1\n",
        "\n",
        "        return aL\n",
        "\n",
        "input_size = 2\n",
        "output_size = K\n",
        "model = FeedforwardNN(input_size, output_size)\n",
        "\n",
        "### Store the input data as a PyTorch tensor ###\n",
        "x_train = torch.tensor(x_train, dtype = torch.float)\n",
        "\n",
        "### One hot encoding ###\n",
        "y_onehot = np.zeros((y_train.size, K))\n",
        "y_onehot[np.arange(y_train.size),y_train] = 1\n",
        "y_onehot = torch.tensor(y_onehot, dtype = torch.float)\n",
        "\n",
        "### Use backpropagation to minimize the cost function using the gradient descent algorithm: ###\n",
        "learning_rate = 1\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "### Cost function: ###\n",
        "cost_func = torch.nn.MSELoss()\n",
        "\n",
        "N_epochs = 10000 # number of times to run gradient descent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOtexsTq-EfC"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ng0FlPl-YNf"
      },
      "source": [
        "epoch_list    = []\n",
        "cost_training = []\n",
        "acc_training  = []\n",
        "\n",
        "############ Function for plotting: ############\n",
        "def updatePlot():\n",
        "\n",
        "    ### Generate coordinates covering the whole plane: ###\n",
        "    padding = 0.1\n",
        "    spacing = 0.02\n",
        "    x1_min, x1_max = x_train[:, 0].min() - padding, x_train[:, 0].max() + padding\n",
        "    x2_min, x2_max = x_train[:, 1].min() - padding, x_train[:, 1].max() + padding\n",
        "    x1_grid, x2_grid = np.meshgrid(np.arange(x1_min, x1_max, spacing),\n",
        "                         np.arange(x2_min, x2_max, spacing))\n",
        "\n",
        "    torch_input = torch.tensor(np.c_[x1_grid.ravel(), x2_grid.ravel()], dtype = torch.float)\n",
        "    NN_output = model(torch_input)\n",
        "    predicted_class = np.argmax(NN_output.detach().numpy(), axis=1)\n",
        "\n",
        "    ### Plot the classifier: ###\n",
        "    plt.subplot(121)\n",
        "    plt.contourf(x1_grid, x2_grid, predicted_class.reshape(x1_grid.shape), K, alpha=0.8)\n",
        "    plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train, s=40)\n",
        "    plt.xlim(x1_grid.min(), x1_grid.max())\n",
        "    plt.ylim(x2_grid.min(), x2_grid.max())\n",
        "    plt.xlabel(r'$x_1$')\n",
        "    plt.ylabel(r'$x_2$')\n",
        "\n",
        "    ### Plot the cost function during training: ###\n",
        "    plt.subplot(222)\n",
        "    plt.plot(epoch_list,cost_training,'o-')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Training cost')\n",
        "\n",
        "    ### Plot the training accuracy: ###\n",
        "    plt.subplot(224)\n",
        "    plt.plot(epoch_list,acc_training,'o-')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Training accuracy')\n",
        "############ End of plotting function ############\n",
        "\n",
        "### Train for several epochs: ###\n",
        "for epoch in range(N_epochs):\n",
        "\n",
        "    optimizer.zero_grad() # sets the gradients to zero (necessary since PyTorch accumulates the gradients)\n",
        "    NN_output = model(x_train) # Neural network output\n",
        "    cost = cost_func(NN_output, y_onehot)\n",
        "    cost.backward() #computes the gradients\n",
        "    optimizer.step() #updating the parameters\n",
        "\n",
        "    ### Update the plot and print results every 500 epochs: ###\n",
        "    if epoch % 500 == 0:\n",
        "        predicted_class = np.argmax(NN_output.detach().numpy(), axis=1)\n",
        "        accuracy = np.mean(predicted_class == y_train)\n",
        "\n",
        "        epoch_list.append(epoch)\n",
        "        cost_training.append(cost.detach().numpy())\n",
        "        acc_training.append(accuracy)\n",
        "\n",
        "        ### Update the plot of the resulting classifier: ###\n",
        "        fig = plt.figure(2,figsize=(10,5))\n",
        "        fig.subplots_adjust(hspace=.3,wspace=.3)\n",
        "        plt.clf()\n",
        "        updatePlot()\n",
        "        display.display(plt.gcf())\n",
        "        print(\"Iteration %d:\\n  Training cost %f\\n  Training accuracy %f\\n\" % (epoch, cost, accuracy) )\n",
        "        display.clear_output(wait=True)\n",
        "        # time.sleep(0.1) #Uncomment this line if you want to slow down the rate of plot updates\n",
        "\n",
        "plt.savefig('spiral_results.pdf', bbox_inches=\"tight\")\n",
        "print(\"Final Training cost %f\\nFinal Training accuracy %f\\n\" % (cost, accuracy) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CwmTP3jHIUA"
      },
      "source": [
        "#Run this cell if you want to save a pdf plot of the results:\n",
        "#files.download('spiral_results.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}